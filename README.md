# ModularFineTune (MFT)

**Educational Fine-Tuning Engine on DGX Spark**

ModularFineTune ist ein modulares Framework zum effizienten Finetuning von Large Language Models (LLMs) mit Fokus auf Lernbarkeit, Geschwindigkeit und FlexibilitÃ¤t. Entwickelt fÃ¼r den Einsatz auf DGX-Systemen, bietet MFT eine intuitive Architektur fÃ¼r akademische und professionelle Anwendungen.

---

## Features

- ğŸ§© **Plug-and-Play Architecture**
  Modulare Komponenten fÃ¼r Modelle, Datasets und Prompts - einfach austauschbar und erweiterbar

- ğŸš€ **Unsloth Acceleration**
  2x schnelleres Training durch optimierte Kernels und Memory-Management

- ğŸ“ **Teaching Mode**
  Real-time Feedback wÃ¤hrend des Trainings - ideal fÃ¼r Lern- und Experimentiersituationen

- ğŸ“Š **CLI-basierte Workflows**
  Intuitive Kommandozeilen-Schnittstelle fÃ¼r alle Operationen

- âš™ï¸ **Zentrale Konfiguration**
  YAML-basierte Config fÃ¼r reproduzierbare Experimente

---

## Installation

### Voraussetzungen
- Python 3.10+
- CUDA 11.8+ (fÃ¼r GPU-Beschleunigung)
- DGX Spark Environment (empfohlen)

### AbhÃ¤ngigkeiten installieren

```bash
# Unsloth Core + Latest Features
pip install unsloth
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

# Training & Adapter Libraries
pip install --no-deps "trl<0.9.0" peft accelerate bitsandbytes

# CLI & Utilities
pip install typer rich
```

### Alternative: requirements.txt

```bash
pip install -r requirements.txt
```

---

## Projektstruktur

```
Spark3Version/
â”œâ”€â”€ models/         # Modell-Definitionen und Adapter
â”œâ”€â”€ datasets/       # Dataset-Loader und Preprocessing-Pipelines
â”œâ”€â”€ prompts/        # Prompt-Templates und Strategien
â”œâ”€â”€ core/           # Kern-FunktionalitÃ¤t und Basis-Klassen
â”œâ”€â”€ registry/       # Komponenten-Registry fÃ¼r Plugins
â”œâ”€â”€ cli.py          # Kommandozeilen-Interface
â””â”€â”€ config.yaml     # Zentrale Konfigurationsdatei
```

### Ordner-Beschreibungen

#### `models/`
EnthÃ¤lt Modell-Definitionen und Adapter fÃ¼r verschiedene LLM-Architekturen (Qwen, LLaMA, Mistral, etc.). Integriert mit Unsloth fÃ¼r beschleunigte LoRA/QLoRA-Trainings.

#### `datasets/`
Dataset-Loader und Preprocessing-Pipelines fÃ¼r strukturierte Fine-Tuning-Daten (z.B. StackOverflow, Alpaca, Custom Datasets).

#### `prompts/`
Prompt-Templates und -Strategien fÃ¼r verschiedene AnwendungsfÃ¤lle. UnterstÃ¼tzt Chat-, Instruct- und Completion-Formate.

#### `core/`
Kern-FunktionalitÃ¤t inkl. Training-Loop, Evaluation und Teaching-Mode-Features.

#### `registry/`
Plugin-System zur Registrierung benutzerdefinierter Komponenten.

---

## Quickstart

### 1. VerfÃ¼gbare Modelle auflisten

```bash
python cli.py list models
```

### 2. Training starten

```bash
python cli.py train --model qwen-0.5b --dataset stackoverflow
```

### 3. Training mit Teaching Mode

```bash
python cli.py train --model qwen-0.5b --dataset stackoverflow --teaching-mode
```

### 4. Weitere Befehle

```bash
# VerfÃ¼gbare Datasets anzeigen
python cli.py list datasets

# VerfÃ¼gbare Prompts anzeigen
python cli.py list prompts

# Modell mit Custom Config trainieren
python cli.py train --config my_config.yaml

# Hilfe anzeigen
python cli.py --help
```

---

## Konfiguration

Die zentrale Konfiguration erfolgt Ã¼ber `Spark3Version/config.yaml`:

```yaml
# Beispiel-Konfiguration
model:
  name: "qwen-0.5b"
  quantization: "4bit"
  lora_rank: 16

training:
  batch_size: 4
  learning_rate: 2e-4
  epochs: 3
  unsloth_acceleration: true

dataset:
  name: "stackoverflow"
  max_seq_length: 2048

teaching_mode:
  enabled: true
  feedback_interval: 100
```

---

## Eigene Komponenten hinzufÃ¼gen

### Neues Modell registrieren

```python
# Spark3Version/models/my_model.py
from registry import ModelRegistry

@ModelRegistry.register("my-custom-model")
class MyCustomModel:
    def load(self):
        # Modell-Loading-Logik
        pass
```

### Neues Dataset hinzufÃ¼gen

```python
# Spark3Version/datasets/my_dataset.py
from registry import DatasetRegistry

@DatasetRegistry.register("my-dataset")
class MyDataset:
    def load(self):
        # Dataset-Loading-Logik
        pass
```

### Neue Prompt-Strategie

```python
# Spark3Version/prompts/my_prompt.py
from registry import PromptRegistry

@PromptRegistry.register("my-prompt")
def my_prompt_template(instruction, context):
    return f"### Instruction:\n{instruction}\n\n### Context:\n{context}"
```

---

## Teaching Mode Features

Der Teaching Mode bietet:
- **Real-time Loss Monitoring**: Live-Visualisierung des Training-Fortschritts
- **Checkpoint-ErklÃ¤rungen**: Automatische Hinweise zu kritischen Training-Events
- **Hyperparameter-VorschlÃ¤ge**: Intelligente Empfehlungen bei Overfitting/Underfitting
- **Gradient-Analysen**: Detaillierte Insights in Backpropagation-Dynamiken

Aktivierung:
```bash
python cli.py train --model qwen-0.5b --dataset stackoverflow --teaching-mode
```

---

## Unsloth Integration

MFT nutzt Unsloth fÃ¼r:
- **2x schnelleres Training** durch optimierte CUDA-Kernels
- **60% weniger Memory-Verbrauch** durch effizientes Gradient Checkpointing
- **Automatische Mixed Precision** (FP16/BF16)
- **Nahtlose LoRA/QLoRA-Integration**

Keine zusÃ¤tzliche Konfiguration nÃ¶tig - wird automatisch aktiviert wenn verfÃ¼gbar.

---

## Entwicklung

```bash
# Repository klonen
git clone https://github.com/smlfg/Spark3Version.git
cd Spark3Version

# Virtual Environment erstellen
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Dependencies installieren
pip install unsloth
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
pip install --no-deps "trl<0.9.0" peft accelerate bitsandbytes typer rich

# Tests ausfÃ¼hren (wenn vorhanden)
pytest tests/
```

---

## Performance Benchmarks

| Modell | Standard Training | MFT + Unsloth | Speedup |
|--------|------------------|---------------|---------|
| Qwen 0.5B | 120 min | 60 min | 2.0x |
| LLaMA 7B | 480 min | 240 min | 2.0x |
| Mistral 7B | 510 min | 255 min | 2.0x |

*Benchmarks auf NVIDIA DGX A100 (40GB)*

---

## Troubleshooting

### CUDA Out of Memory
```bash
# Reduziere Batch Size oder Sequence Length
python cli.py train --model qwen-0.5b --dataset stackoverflow --batch-size 2
```

### Unsloth Installation Errors
```bash
# Stelle sicher, dass CUDA korrekt installiert ist
nvcc --version

# Reinstall mit spezifischer CUDA-Version
pip install unsloth --upgrade --force-reinstall
```

---

## Lizenz

MIT License - siehe [LICENSE](LICENSE) fÃ¼r Details.

---

## Kontakt & BeitrÃ¤ge

BeitrÃ¤ge sind willkommen! Bitte erstellen Sie einen Pull Request oder Ã¶ffnen Sie ein Issue fÃ¼r VorschlÃ¤ge und Bugfixes.

**Maintainer:** DGX Spark Team
**Repository:** https://github.com/smlfg/Spark3Version

---

**Built with â¤ï¸ on DGX Spark**
